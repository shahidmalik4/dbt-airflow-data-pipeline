## End-to-End Analytics Engineering Platform  

[![Python](https://img.shields.io/badge/python-3.11-blue?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=apacheairflow&logoColor=white)](https://airflow.apache.org/)
[![dbt](https://img.shields.io/badge/dbt-FF694B?style=for-the-badge&logo=dbt&logoColor=white)](https://www.getdbt.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)
[![FastAPI](https://img.shields.io/badge/FastAPI-009485?style=for-the-badge&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![Metabase](https://img.shields.io/badge/Metabase-EC4A3F?style=for-the-badge&logo=metabase&logoColor=white)](https://www.metabase.com/)
[![License](https://img.shields.io/badge/license-MIT-green?style=for-the-badge)](https://opensource.org/licenses/MIT)


A **production-style analytics engineering project** that demonstrates how raw data moves from ingestion â†’ transformation â†’ orchestration â†’ APIs â†’ BI dashboards using modern data stack tools.

This project is designed to **mirror real-world data platforms** used by Analytics Engineers, Data Engineers, and BI teams.

---

## ğŸ§  Project Overview

This repository implements a **complete analytics workflow**:

1. **Data Ingestion** â€“ Load TPCH-style data into Postgres  
2. **Transformations (dbt)** â€“ Build staging, marts, and analytics models  
3. **Orchestration (Airflow)** â€“ Schedule ingestion and dbt runs  
4. **Serving Layer (FastAPI)** â€“ Expose analytics KPIs via REST APIs  
5. **BI & Dashboards (Metabase)** â€“ Explore data visually  

The result is a **fully containerized analytics platform** you can run locally.

---

## ğŸ§  Skills Demonstrated

- Analytics Engineering best practices
- Data modeling (facts, dimensions, KPIs)
- dbt testing & documentation
- Workflow orchestration with Airflow
- API-driven analytics delivery
- BI enablement with Metabase
- Docker-based local data platforms

---

## ğŸ—ï¸ File Structure
```
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ my_dag.py
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”œâ”€â”€ marts/
â”‚   â”‚   â”‚   â””â”€â”€ analytics/
â”‚   â”‚   â”œâ”€â”€ macros/
â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml
â”œâ”€â”€ fastapi/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ load_faker_data.py
â”‚   â””â”€â”€ postgres_data_load.py
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ setup_project.sh
â”œâ”€â”€ start_services.sh
â”œâ”€â”€ sql-scripts/
â”‚   â”œâ”€â”€ init-airflow.sql
â”‚   â””â”€â”€ init-warehouse.sql
â””â”€â”€ TPCH/
    â””â”€â”€ *.tbl

```
---

## ğŸ”§ Tech Stack

| Layer            | Tool |
|------------------|------|
| Database         | PostgreSQL |
| Transformations  | dbt |
| Orchestration    | Apache Airflow |
| API Layer        | FastAPI |
| BI / Dashboards  | Metabase |
| Containerization | Docker & Docker Compose |

---

## ğŸ“Š dbt Features Used
- Staging â†’ Marts â†’ Analytics layers
- Tests & custom tests
- Incremental models
- SCD snapshots
- Documentation generation

---

## â±ï¸ Airflow Orchestration

Airflow orchestrates the entire pipeline:

- Warehouse readiness checks
- Ingestion tasks
- dbt run & dbt test
- Retry handling and logging

Each step is **modular, observable, and production-aligned**.

---

## Getting Started / Setup Instructions

1. **Clone the repository**
```bash
git clone https://github.com/shahidmalik4/dbt-airflow-data-pipeline.git
cd dbt-airflow-data-pipeline
```

2. **Set up the Python environment**
```
python -m venv venv
source venv/bin/activate
.\venv\Scripts\Activate.ps1 (on windows - powershell)
.\venv\Scripts\activate.bat (on windows - command prompt)
```

3. **Make scripts executable (Linux / macOS only)**
```
chmod +x ./setup_project.sh
chmod +x ./start_services.sh
```

4. **Run Services (will pull all docker images, build, and run the containers required for the project)**
```
./setup_project.sh
```

5. **dbt Commands (Inside airflow-webserver Container)**
```
dbt debug
dbt docs generate
```

---

## ğŸŒ Access Services

| Service  | URL                        |
|----------|----------------------------|
| Airflow  | [http://localhost:8080](http://localhost:8080) |
| DBT Docs  | [http://localhost:8081](http://localhost:8081) |
| FastAPI  | [http://localhost:8000/docs](http://localhost:8000/docs) |
| Metabase | [http://localhost:3000](http://localhost:3000) |

---

## FastAPI Swagger UI
![My Image](include/fastapi.png)


## Metabase Dashboard
![My Image](include/dashboard.png)


