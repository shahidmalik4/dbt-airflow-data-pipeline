docker compose run --rm airflow-webserver airflow db init

docker compose run --rm airflow-webserver airflow connections add postgres_local \
    --conn-uri "postgresql://admin:admin@postgres_analytics:5432/warehouse"

SELECT table_schema, table_name, table_type FROM information_schema.tables WHERE table_schema NOT IN ('pg_catalog', 'information_schema') AND table_type IN ('BASE TABLE', 'VIEW') ORDER BY table_schema, table_name;

DROP SCHEMA analytics CASCADE; DROP SCHEMA marts CASCADE; DROP SCHEMA staging CASCADE; DROP SCHEMA raw CASCADE;

CREATE SCHEMA analytics; CREATE SCHEMA marts; CREATE SCHEMA staging;

sudo chmod -R a+r ./TPCH

warehouse=# \d marts.dim_customer;
SELECT * FROM marts.dim_customer LIMIT 0;

dbt-airflow-data-pipeline_pgdata
Postgres catalog inside pgdata got corrupted from previous runs.
docker volume rm dbt-airflow-data-pipeline_pgdata

docker compose up fastapi
docker compose up airflow-webserver airflow-scheduler -d
<<<<<<< HEAD
=======


export UID=$(id -u)
export GID=$(id -g)

docker compose exec airflow-webserver bash

cd dbt/dbt_project
dbt debug
dbt run
dbt docs generate
>>>>>>> ffc3e9c (added dbt analytics models)
